{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f250e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9067a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i_golov/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch_geometric/utils/convert.py:4: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  import scipy.sparse\n"
     ]
    }
   ],
   "source": [
    "from processors.processors_diffms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500608a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "def smiles_to_fingerprint(smiles: str, n_bits: int, radius: int) -> np.ndarray:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None: \n",
    "                print(\"Molecula formula not defined\")\n",
    "                return np.zeros(n_bits, dtype=np.float32)\n",
    "        mfpgen = rdFingerprintGenerator.GetMorganGenerator(fpSize=n_bits, radius=radius)\n",
    "        fp = mfpgen.GetFingerprint(mol)\n",
    "        return np.array(fp, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e279a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the spec_features dictionary to a batch format (adding batch dimension)\n",
    "def prepare_features(spec_features_dict):\n",
    "    \"\"\"Converts features from numpy arrays to PyTorch tensors.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    features['num_peaks'] = len(spec_features_dict['peak_type'])\n",
    "    \n",
    "    # Convert arrays to tensors\n",
    "    features['types'] = torch.tensor(spec_features_dict['peak_type'], dtype=torch.long)\n",
    "    features['form_vec'] = torch.tensor(spec_features_dict['form_vec'], dtype=torch.float)\n",
    "    features['ion_vec'] = torch.tensor(spec_features_dict['ion_vec'], dtype=torch.long)\n",
    "    features['intens'] = torch.tensor(spec_features_dict['frag_intens'], dtype=torch.float)\n",
    "    features['instruments'] = torch.tensor(spec_features_dict['instrument'], dtype=torch.long) \n",
    "    features['num_peaks'] = torch.tensor(features['num_peaks'] , dtype=torch.long) \n",
    "    \n",
    "    if 'magma_fps' in spec_features_dict:\n",
    "         # Ensure magma_fps is float for consistency, handle -1 if needed\n",
    "         magma_fps_np = spec_features_dict['magma_fps']\n",
    "         features['magma_fps'] = torch.tensor(magma_fps_np, dtype=torch.float)\n",
    "         features['magma_aux_loss'] = spec_features_dict['magma_aux_loss']\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12cb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def spectra_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collates a list of dictionaries from SpectraDataset into a padded batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of dictionaries, where each dict is an output\n",
    "                      from SpectraDataset.__getitem__.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing batched and padded tensors,\n",
    "              or None if the batch is empty after filtering.\n",
    "    \"\"\"\n",
    "    # Filter out None items resulting from errors in __getitem__\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    target_fps = torch.stack([item['target_fp'] for item in batch], dim=0)\n",
    "    instruments = torch.stack([item['instruments'] for item in batch], dim=0)\n",
    "    \n",
    "    # --- Handle sequence padding ---\n",
    "    # Get sequence lengths (num_peaks) - from python ints\n",
    "    num_peaks = torch.tensor([item['num_peaks'] for item in batch], dtype=torch.long)\n",
    "    max_len = num_peaks.max().item() if len(num_peaks) > 0 else 0 # Handle empty batch case\n",
    "    \n",
    "    # Prepare lists of tensors for pad_sequence\n",
    "    types_list = [item['types'] for item in batch]\n",
    "    form_vec_list = [item['form_vec'] for item in batch]\n",
    "    ion_vec_list = [item['ion_vec'] for item in batch]\n",
    "    intens_list = [item['intens'] for item in batch]\n",
    "    \n",
    "    # Pad sequences: batch_first=True gives [batch_size, max_len, ...]\n",
    "    # Use 0 for padding value, adjust if a different value is semantically better\n",
    "    batched_types = pad_sequence(types_list, batch_first=True, padding_value=0)\n",
    "    batched_form_vecs = pad_sequence(form_vec_list, batch_first=True, padding_value=0.0)\n",
    "    batched_ion_vecs = pad_sequence(ion_vec_list, batch_first=True, padding_value=0)\n",
    "    batched_intens = pad_sequence(intens_list, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    mask = torch.arange(max_len)[None, :] < num_peaks[:, None]\n",
    "    \n",
    "    final_batch = {\n",
    "        'target_fp': target_fps,\n",
    "        'instruments': instruments,\n",
    "        'num_peaks': num_peaks,\n",
    "        'types': batched_types,\n",
    "        'form_vec': batched_form_vecs,\n",
    "        'ion_vec': batched_ion_vecs,\n",
    "        'intens': batched_intens,\n",
    "        'mask': mask\n",
    "    }\n",
    "    \n",
    "    if 'magma_fps' in batch[0]:\n",
    "        magma_fps_list = [item['magma_fps'] for item in batch]\n",
    "        # Pad MAGMA fingerprints. Using 0.0 as padding value.\n",
    "        # The original data uses -1 for missing FPs, padding adds 0s.\n",
    "        # Ensure your model handles both -1 (missing) and 0 (padding or inactive bit) appropriately.\n",
    "        batched_magma_fps = pad_sequence(magma_fps_list, batch_first=True, padding_value=0.0)\n",
    "        final_batch['magma_fps'] = batched_magma_fps\n",
    "        # Carry over the boolean flag (assuming it's the same for the whole batch)\n",
    "        final_batch['magma_aux_loss'] = batch[0]['magma_aux_loss']\n",
    "        \n",
    "    return final_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d74abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectraDataset(Dataset):\n",
    "    \"\"\"Dataset for loading spectra and SMILES from a CSV file.\"\"\"\n",
    "    def __init__(self, data_file_path, spectrum_processor, target_fp_size, is_train=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file_path (str): Path to the CSV file.\n",
    "            spectrum_processor (SpectrumProcessor): Instance to process spectra.\n",
    "            target_fp_size (int): The desired size of the target fingerprint.\n",
    "            is_train (bool): Flag indicating if this is for training (enables augmentation).\n",
    "        \"\"\"\n",
    "        self.processor = spectrum_processor\n",
    "        self.target_fp_size = target_fp_size\n",
    "        self.is_train = is_train\n",
    "        self.morgan_radius = 2\n",
    "        \n",
    "        try:\n",
    "            self.data = pd.read_csv(data_file_path)\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_cols = ['spec', 'smiles', 'extracted_spectral_info']\n",
    "            if not all(col in self.data.columns for col in required_cols):\n",
    "                raise ValueError(f\"CSV must contain columns: {required_cols}\")\n",
    "            print(f\"Loaded {len(self.data)} records from {data_file_path}\")\n",
    "            \n",
    "            # TODO implement the SMILES validation and filtering\n",
    "            # Optional: Pre-filter invalid SMILES to avoid errors during training\n",
    "            # self.data['valid_smiles'] = self.data['smiles'].apply(lambda x: Chem.MolFromSmiles(str(x)) is not None)\n",
    "            # initial_len = len(self.data)\n",
    "            # self.data = self.data[self.data['valid_smiles']].reset_index(drop=True)\n",
    "            # print(f\"Filtered out {initial_len - len(self.data)} invalid SMILES.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found at {data_file_path}\")\n",
    "            self.data = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing CSV {data_file_path}: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed spectral features\n",
    "                  and the target fingerprint. Returns None if data is invalid.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Extract data, ensuring correct types\n",
    "        spec_id = str(row['spec'])\n",
    "        smiles = str(row['smiles'])\n",
    "        raw_spec_json = row['extracted_spectral_info'] \n",
    "\n",
    "        # --- Generate Target Fingerprint ---\n",
    "        target_fp = smiles_to_fingerprint(smiles, self.target_fp_size, self.morgan_radius)\n",
    "        target_fp = torch.tensor(target_fp)\n",
    "     \n",
    "        # --- Process Spectrum ---\n",
    "        spec_features = self.processor.process_raw_spectrum(\n",
    "            raw_spec_json, spec_id=spec_id, train_mode=self.is_train\n",
    "        )\n",
    "        \n",
    "        spec_features = prepare_features(spec_features)\n",
    "        \n",
    "        # --- Combine features and target --- \n",
    "        item = {**spec_features, 'target_fp': target_fp}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15105a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4850b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/production_ready_data/train/spectrs/\"\n",
    "TRAIN_CSV = Path(DATA_DIR) / \"MassSpecGym_fixed.csv\" \n",
    "OUTPUT_SIZE = 4096  \n",
    "HIDDEN_SIZE = 256 \n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4    \n",
    "MAGMA_MODULO = 2048 \n",
    "SPECTRA_DROPOUT = 0.1 \n",
    "TOP_LAYERS = 2 \n",
    "USE_MAGMA_AUX_LOSS = True \n",
    "FORM_EMBEDDER = \"float\"\n",
    "MAGMA_FOLDER = '../../data/raw/msg_diffms/magma_outputs/magma_tsv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a78741",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1.0e-6\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "USE_MAGMA_AUX_LOSS = True \n",
    "MAGMA_LOSS_WEIGHT = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742987e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 231104 MAGMA files\n"
     ]
    }
   ],
   "source": [
    "processor_train = SpectrumProcessor(\n",
    "    augment_data=True,\n",
    "    cls_type=\"ms1\",\n",
    "    max_peaks=500,\n",
    "    magma_modulo=MAGMA_MODULO,\n",
    "    magma_aux_loss=USE_MAGMA_AUX_LOSS,\n",
    "    magma_folder=MAGMA_FOLDER if USE_MAGMA_AUX_LOSS else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59efe99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 231104 records from ../../data/production_ready_data/train/spectrs/MassSpecGym_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpectraDataset(TRAIN_CSV, processor_train, OUTPUT_SIZE, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b22c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_peaks': tensor(8), 'types': tensor([0, 0, 0, 0, 0, 0, 0, 3]), 'form_vec': tensor([[13., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 7.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [13., 13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 7.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 6.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [14., 15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 7.,  7.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [16., 17.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  4.,\n",
      "          0.,  0.,  0.,  0.]]), 'ion_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'intens': tensor([0.5911, 0.4952, 0.2122, 0.5961, 1.0000, 0.8572, 0.2830, 1.0000]), 'instruments': tensor(0), 'magma_fps': tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]]), 'magma_aux_loss': True, 'target_fp': tensor([0., 0., 0.,  ..., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d912a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created with 3611 batches.\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=spectra_collate_fn,\n",
    "        )\n",
    "print(f\"Train DataLoader created with {len(train_loader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fb4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c37affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.spectra_encoder import SpectraEncoder, SpectraEncoderGrowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectraEncoder(\n",
    "    form_embedder=FORM_EMBEDDER,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    spectra_dropout=SPECTRA_DROPOUT,\n",
    "    top_layers=TOP_LAYERS,\n",
    "    magma_modulo=MAGMA_MODULO,\n",
    "    peak_attn_layers=4,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1469558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_main = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1cb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculates BCE loss, ignoring targets where value is -1.\"\"\"\n",
    "    mask = target != -1\n",
    "    target_masked = target[mask]\n",
    "    pred_masked = pred[mask]\n",
    "    if target_masked.numel() == 0: # Handle cases where mask removes all elements\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True) # Return zero loss but allow grad flow\n",
    "    loss = nn.BCELoss(reduction='mean')(pred_masked, target_masked)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f308fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5590cd6a6cc448cb93ec6b3d7ec5539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/3611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m total_main_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     10\u001b[0m     batch_gpu \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m item\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m, in \u001b[0;36mSpectraDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m target_fp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target_fp)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# --- Process Spectrum ---\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m spec_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_raw_spectrum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_spec_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_train\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m spec_features \u001b[38;5;241m=\u001b[39m prepare_features(spec_features)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# --- Combine features and target --- \u001b[39;00m\n",
      "File \u001b[0;32m~/Spectrum_Structure_prediction/Spectrum-to-Molecular/notebooks/mist_encoder/processors/processors_diffms.py:109\u001b[0m, in \u001b[0;36mSpectrumProcessor.process_raw_spectrum\u001b[0;34m(self, raw_spectral_json, spec_id, train_mode)\u001b[0m\n\u001b[1;32m    106\u001b[0m         peak_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_peak_dict(peak_dict)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Generate feature dictionary\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeak_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/Spectrum_Structure_prediction/Spectrum-to-Molecular/notebooks/mist_encoder/processors/processors_diffms.py:314\u001b[0m, in \u001b[0;36mSpectrumProcessor._generate_features\u001b[0;34m(self, peak_dict, spec_name)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Add in chemical formulae\u001b[39;00m\n\u001b[1;32m    312\u001b[0m root \u001b[38;5;241m=\u001b[39m peak_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot_form\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 314\u001b[0m forms_vec \u001b[38;5;241m=\u001b[39m [utils\u001b[38;5;241m.\u001b[39mformula_to_dense(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m peak_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrags\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(forms_vec) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     mz_vec \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Spectrum_Structure_prediction/Spectrum-to-Molecular/notebooks/mist_encoder/processors/processors_diffms.py:314\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Add in chemical formulae\u001b[39;00m\n\u001b[1;32m    312\u001b[0m root \u001b[38;5;241m=\u001b[39m peak_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot_form\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 314\u001b[0m forms_vec \u001b[38;5;241m=\u001b[39m [\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformula_to_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m peak_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrags\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(forms_vec) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     mz_vec \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Spectrum_Structure_prediction/Spectrum-to-Molecular/notebooks/mist_encoder/utils/chem_utils.py:267\u001b[0m, in \u001b[0;36mformula_to_dense\u001b[0;34m(chem_formula)\u001b[0m\n\u001b[1;32m    265\u001b[0m     dense_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(element_to_position))\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     dense_vec \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dense_vec\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_main_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    \n",
    "    for item in pbar:\n",
    "        batch_gpu = {}\n",
    "        for key, value in item.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_gpu[key] = value.to(DEVICE)\n",
    "                else:\n",
    "                    batch_gpu[key] = value \n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        output, aux_outputs = model(batch_gpu)\n",
    "        target_fp = batch_gpu['target_fp']\n",
    "        target_fp = target_fp.view_as(output).float()\n",
    "        main_loss = criterion_main(output, target_fp)\n",
    "        \n",
    "        aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        if USE_MAGMA_AUX_LOSS and 'pred_frag_fps' in aux_outputs and 'magma_fps' in batch_gpu and batch_gpu['magma_fps'].numel() > 0:\n",
    "            pred_frag_fps = aux_outputs['pred_frag_fps']\n",
    "            target_magma_fps = batch_gpu['magma_fps'] \n",
    "            if pred_frag_fps.shape == target_magma_fps.shape:\n",
    "                pred_frag_fps_sig = torch.sigmoid(pred_frag_fps) # logits output\n",
    "                aux_loss = masked_bce_loss(pred_frag_fps_sig, target_magma_fps.float())\n",
    "        \n",
    "        # --- Combine Losses ---\n",
    "        total_loss = main_loss + MAGMA_LOSS_WEIGHT * aux_loss\n",
    "        \n",
    "         # --- Backward Pass & Optimize ---\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        current_main_loss = main_loss.item()\n",
    "        current_aux_loss = aux_loss.item() \n",
    "        total_main_loss += current_main_loss\n",
    "        \n",
    "        pbar.set_postfix(main_loss=f\"{current_main_loss:.4f}\", aux_loss=f\"{current_aux_loss:.4f}\")\n",
    "\n",
    "    pbar.close()\n",
    "        \n",
    "    final_avg_main_loss = total_main_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Avg Main Loss: {final_avg_main_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a472c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0273a147047e408bb3960252dc3820c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/3611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0322428.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0000152.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0000151.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0378754.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0191256.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0000155.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0000156.magma: 'float' object has no attribute 'split'\n",
      "Error processing MAGMA file ../../data/raw/msg_diffms/magma_outputs/magma_tsv/MassSpecGymID0191254.magma: 'float' object has no attribute 'split'\n",
      "--- Epoch 1 Training Finished ---\n",
      "Average Training Loss: 0.0000, Avg Main Loss: 0.0000, Avg Aux Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5e3bfff19445f1a281e9b84b1af258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50:   0%|          | 0/3611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Wrap train_loader with tqdm for a progress bar\u001b[39;00m\n\u001b[1;32m     10\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar): \u001b[38;5;66;03m# Iterate over the tqdm object\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Move batch to device (handle non-tensor data appropriately)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     batch_gpu \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spectrum-to-molecular-ECHCqm5f-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_main_loss = 0\n",
    "    total_aux_loss = 0\n",
    "\n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "    for i, batch in enumerate(progress_bar): # Iterate over the tqdm object\n",
    "        # Move batch to device (handle non-tensor data appropriately)\n",
    "        batch_gpu = {}\n",
    "        for key, value in batch.items():\n",
    "             if isinstance(value, torch.Tensor):\n",
    "                 batch_gpu[key] = value.to(DEVICE)\n",
    "             else:\n",
    "                 batch_gpu[key] = value # Keep non-tensors like names, flags as they are\n",
    "\n",
    "        # Skip batch if essential tensors are empty after collation/filtering\n",
    "        if not batch_gpu.get('form_vec', torch.empty(0)).numel() or not batch_gpu.get('target_fp', torch.empty(0)).numel():\n",
    "             # print(f\"Skipping empty batch {i}\") # Optional: tqdm might make this less necessary\n",
    "             continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output, aux_outputs = model(batch_gpu) # Pass the gpu batch\n",
    "     \n",
    "\n",
    "         # Calculate Main Loss\n",
    "        target_fp = batch_gpu['target_fp']\n",
    "        main_loss = criterion_main(output, target_fp)\n",
    "\n",
    "        # Calculate Auxiliary Loss (if enabled)\n",
    "        aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        if USE_MAGMA_AUX_LOSS and 'pred_frag_fps' in aux_outputs and 'magma_fps' in batch_gpu and batch_gpu['magma_fps'].numel() > 0:\n",
    "            pred_frag_fps = aux_outputs['pred_frag_fps'] # Shape: [batch, (num_peaks,) magma_modulo]\n",
    "            target_magma_fps = batch_gpu['magma_fps']   # Shape: [batch, num_peaks, magma_modulo]\n",
    "\n",
    "            # --- IMPORTANT ---\n",
    "            # Ensure pred_frag_fps and target_magma_fps align.\n",
    "            # If pred_frag_fps is [batch, magma_modulo] (global prediction),\n",
    "            # you cannot directly compare it to per-peak target_magma_fps.\n",
    "            # The MIST paper/code needs clarification on how aux loss is computed.\n",
    "            # Assuming pred_frag_fps is per-peak for this example:\n",
    "            # Shape: [batch, num_peaks, magma_modulo]\n",
    "            # Make sure the shapes match before calling masked_bce_loss.\n",
    "            # If shapes mismatch, you need to adapt the model or loss calculation.\n",
    "\n",
    "            # Example check (adjust based on actual model output shape):\n",
    "            if pred_frag_fps.shape == target_magma_fps.shape:\n",
    "                 # Apply sigmoid if fragment_predictor doesn't have one\n",
    "                 pred_frag_fps_sig = torch.sigmoid(pred_frag_fps) # Assuming logits output\n",
    "                 aux_loss = masked_bce_loss(pred_frag_fps_sig, target_magma_fps)\n",
    "            else:\n",
    "                 # Print warning less frequently or outside tqdm loop if too noisy\n",
    "                 if i % 100 == 0: # Example: Print only every 100 steps\n",
    "                     print(f\"\\nWarning: Shape mismatch for MAGMA loss. Pred: {pred_frag_fps.shape}, Target: {target_magma_fps.shape}. Skipping aux loss.\")\n",
    "                 aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "\n",
    "        # Combine Losses\n",
    "        total_loss = main_loss + MAGMA_LOSS_WEIGHT * aux_loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        try:\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\nError during backward/step on batch {i}: {e}\") # Add newline\n",
    "            # Potentially clear gradients if step failed\n",
    "            optimizer.zero_grad()\n",
    "            continue # Skip this batch update\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_main_loss += main_loss.item()\n",
    "        if USE_MAGMA_AUX_LOSS:\n",
    "            total_aux_loss += aux_loss.item()\n",
    "\n",
    "        # Update tqdm progress bar description with current average losses\n",
    "        avg_loss = total_train_loss / (i + 1)\n",
    "        avg_main_loss = total_main_loss / (i + 1)\n",
    "        avg_aux_loss = total_aux_loss / (i + 1) if USE_MAGMA_AUX_LOSS and total_aux_loss > 0 else 0 # Avoid division by zero if aux loss wasn't used yet\n",
    "        progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", main=f\"{avg_main_loss:.4f}\", aux=f\"{avg_aux_loss:.4f}\")\n",
    "\n",
    "        # Optional: Keep less frequent print statements if needed, but tqdm postfix is often sufficient\n",
    "        # if (i + 1) % 100 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Avg Loss: {avg_loss:.4f}, Avg Main Loss: {avg_main_loss:.4f}, Avg Aux Loss: {avg_aux_loss:.4f}')\n",
    "\n",
    "    # Ensure the progress bar finishes cleanly\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Print final epoch stats after the loop\n",
    "    final_avg_loss = total_train_loss / len(train_loader)\n",
    "    final_avg_main_loss = total_main_loss / len(train_loader)\n",
    "    final_avg_aux_loss = total_aux_loss / len(train_loader) if USE_MAGMA_AUX_LOSS and len(train_loader) > 0 else 0\n",
    "    print(f'--- Epoch {epoch+1} Training Finished ---')\n",
    "    print(f'Average Training Loss: {final_avg_loss:.4f}, Avg Main Loss: {final_avg_main_loss:.4f}, Avg Aux Loss: {final_avg_aux_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrum-to-molecular-ECHCqm5f-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
