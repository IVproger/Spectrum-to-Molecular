{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce23a91",
   "metadata": {},
   "source": [
    "**1. Identifying the Target and Loss Function**\n",
    "\n",
    "*   **Main Output (`output`):** The `SpectraEncoder`'s final output comes from `spectra_predictor`, which ends with `nn.Linear(hidden_size, output_size)` followed by `nn.Sigmoid()`. The `output_size` (e.g., 4096) and the Sigmoid activation strongly suggest the model predicts a **binary fingerprint** representation of the molecule corresponding to the input spectrum. The Sigmoid squashes the output between 0 and 1, suitable for representing the probability of each bit being 'on'.\n",
    "*   **Target for Main Output:** The target should be the actual molecular fingerprint (e.g., Morgan fingerprint, RDKit fingerprint) derived from the molecule's SMILES string (like `CC(=O)NC@@HC2=CC(=CC(=O)O2)OC` for `MassSpecGymID0000001`). You'll need a function (likely using RDKit) to convert SMILES strings into fixed-size binary vectors of length `output_size`.\n",
    "*   **Loss for Main Output:** Given the Sigmoid output and binary target, the standard loss function is **Binary Cross-Entropy (BCE)**. You can use `torch.nn.BCELoss`.\n",
    "*   **Auxiliary Output (`pred_frag_fps`):** The `fragment_predictor` outputs a tensor of size `magma_modulo` (e.g., 2048). This corresponds to the predicted MAGMA fingerprints for the spectral peaks.\n",
    "*   **Target for Auxiliary Output:** The target is the `magma_fps` tensor generated by the `SpectrumProcessor`. Note that this tensor contains 0s, 1s, and -1s (where -1 indicates missing data).\n",
    "*   **Loss for Auxiliary Output:** You need a BCE-like loss that can handle the masked values (-1). A common approach is to calculate BCE only for entries where the target is not -1. This is often referred to as a **Masked BCE Loss**. If `magma_aux_loss` in your `SpectrumProcessor` is `True`, this loss should be calculated and added (potentially with a weighting factor) to the main loss.\n",
    "\n",
    "**2. Searching the Project (Conceptual)**\n",
    "\n",
    "Based on standard practices and the code provided:\n",
    "\n",
    "*   **Loss Functions:** Look for files like `losses.py`, `train.py`, or `main.py`. You'd expect to find instantiations of `nn.BCELoss` and potentially a custom function for the masked BCE loss for the MAGMA fingerprints.\n",
    "*   **Metrics:** Evaluation often uses fingerprint similarity metrics like the **Tanimoto coefficient (Jaccard index)**. Look for functions calculating this in training/validation loops or utility files.\n",
    "*   **Data Loading:** Files like `dataset.py` or `data_loader.py` would define a PyTorch `Dataset` class. This class would:\n",
    "    *   Read your input data (e.g., the TSV line).\n",
    "    *   Parse the SMILES string and spectral data.\n",
    "    *   Use the `SpectrumProcessor` to process the spectrum.\n",
    "    *   Use a cheminformatics library (like RDKit) to generate the target fingerprint from the SMILES string.\n",
    "    *   Return a dictionary containing processed spectrum features (like `spec_features`) and the target fingerprint.\n",
    "*   **Training Loop:** `train.py` or `main.py` would contain the core logic: iterating through epochs and batches, performing forward/backward passes, calculating combined loss, updating weights, and logging metrics.\n",
    " \n",
    "**Key Implementation Points:**\n",
    "\n",
    "1.  **`smiles_to_fingerprint`:** Implement this using RDKit or another cheminformatics library to match the `output_size`.\n",
    "2.  **`SpectraDataset`:** Adapt the data loading (`pd.read_csv`) to your specific input file format. Ensure it correctly extracts SMILES and the raw spectral JSON/dict.\n",
    "3.  **`collate_fn`:** This is crucial and complex. It needs to correctly pad sequences of varying lengths (like `form_vec`, `peak_type`) and create an attention mask that the `FormulaTransformer` likely requires. The implementation provided is a basic example and might need significant adjustments based on how `FormulaTransformer` handles padded input and what padding values are appropriate.\n",
    "4.  **`FormulaTransformer`:** The `DummyFormulaTransformer` needs to be replaced with the actual implementation from `models.modules`. The forward pass within `SpectraEncoder` might need adjustments based on the exact output shapes and meaning of the `FormulaTransformer`'s output (`encoder_output`, `aux_out[\"peak_tensor\"]`). Pay close attention to whether outputs are per-peak or global ([CLS] token style).\n",
    "5.  **MAGMA Auxiliary Loss:** The comparison between `pred_frag_fps` and `target_magma_fps` needs careful handling. Ensure their shapes align and represent the same thing (e.g., both per-peak fingerprints). If `pred_frag_fps` is a single vector per spectrum, you might need to aggregate `target_magma_fps` or modify the model architecture/loss.\n",
    "6.  **Paths:** Replace placeholder paths like `\"path/to/your/...\"` with your actual file locations.\n",
    "7.  **Dependencies:** Ensure you have `torch`, `numpy`, `pandas`, and `rdkit-pypi` installed.\n",
    "\n",
    "This pipeline provides a structural foundation. You'll need to integrate your specific data, the actual `FormulaTransformer` module, and potentially refine the collation and loss calculations based on the MIST paper's details or further code exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming your input is like the TSV line\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assume these are defined in your project ---\n",
    "# from models.mist_encoder import SpectraEncoder # Or SpectraEncoderGrowing\n",
    "# from data.spectrum_processor import SpectrumProcessor\n",
    "# from utils import collate_fn # A function to batch your processed data\n",
    "# --- Using the classes provided in the prompt ---\n",
    "from typing import Tuple\n",
    "from models import modules # Assuming modules.py exists as per your encoder code\n",
    "\n",
    "class SpectraEncoder(nn.Module):\n",
    "    \"\"\"SpectraEncoder.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        form_embedder: str = \"float\",\n",
    "        output_size: int = 4096,\n",
    "        hidden_size: int = 50,\n",
    "        spectra_dropout: float = 0.0,\n",
    "        top_layers: int = 1,\n",
    "        refine_layers: int = 0,\n",
    "        magma_modulo: int = 2048,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(SpectraEncoder, self).__init__()\n",
    "        # --- Using dummy FormulaTransformer for demonstration ---\n",
    "        # Replace with your actual modules.FormulaTransformer\n",
    "        class DummyFormulaTransformer(nn.Module):\n",
    "            def __init__(self, hidden_size, **kwargs):\n",
    "                super().__init__()\n",
    "                self.dummy_layer = nn.Linear(100, hidden_size) # Input size is arbitrary placeholder\n",
    "                self.hidden_size = hidden_size\n",
    "            def forward(self, batch, return_aux=False):\n",
    "                # Dummy forward: needs actual implementation based on FormulaTransformer\n",
    "                # This needs to process batch['form_vec'], batch['peak_type'], etc.\n",
    "                # Returning dummy tensors of expected shapes\n",
    "                batch_size = batch['form_vec'].shape[0] # Assuming batching adds a dimension\n",
    "                num_peaks = batch['form_vec'].shape[1]\n",
    "                dummy_encoder_output = torch.randn(batch_size, self.hidden_size)\n",
    "                dummy_peak_tensor = torch.randn(batch_size, num_peaks, self.hidden_size)\n",
    "                aux_out = {\"peak_tensor\": dummy_peak_tensor}\n",
    "                if return_aux:\n",
    "                    return dummy_encoder_output, aux_out\n",
    "                else:\n",
    "                    return dummy_encoder_output\n",
    "\n",
    "        spectra_encoder_main = DummyFormulaTransformer( # Replace with actual modules.FormulaTransformer\n",
    "            hidden_size=hidden_size,\n",
    "            spectra_dropout=spectra_dropout,\n",
    "            form_embedder=form_embedder,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # ... rest of the __init__ code from your prompt ...\n",
    "        fragment_pred_parts = []\n",
    "        for _ in range(top_layers - 1):\n",
    "            fragment_pred_parts.append(nn.Linear(hidden_size, hidden_size))\n",
    "            fragment_pred_parts.append(nn.ReLU())\n",
    "            fragment_pred_parts.append(nn.Dropout(spectra_dropout))\n",
    "        fragment_pred_parts.append(nn.Linear(hidden_size, magma_modulo))\n",
    "        fragment_predictor = nn.Sequential(*fragment_pred_parts)\n",
    "\n",
    "        top_layer_parts = []\n",
    "        for _ in range(top_layers - 1):\n",
    "            top_layer_parts.append(nn.Linear(hidden_size, hidden_size))\n",
    "            top_layer_parts.append(nn.ReLU())\n",
    "            top_layer_parts.append(nn.Dropout(spectra_dropout))\n",
    "        top_layer_parts.append(nn.Linear(hidden_size, output_size))\n",
    "        top_layer_parts.append(nn.Sigmoid())\n",
    "        spectra_predictor = nn.Sequential(*top_layer_parts)\n",
    "        self.spectra_encoder = nn.ModuleList([spectra_encoder_main, fragment_predictor, spectra_predictor])\n",
    "\n",
    "    def forward(self, batch: dict) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Assuming batch contains tensors now, not numpy arrays\n",
    "        encoder_output, aux_out = self.spectra_encoder[0](batch, return_aux=True)\n",
    "\n",
    "        # Need to handle the fact that peak_tensor might vary in num_peaks per batch item\n",
    "        # Often, the transformer output corresponding to the [CLS] token is used,\n",
    "        # or pooling is applied over peak_tensor. Adjusting based on common practice.\n",
    "        # Let's assume encoder_output is the [CLS] token embedding [batch_size, hidden_size]\n",
    "        # And aux_out[\"peak_tensor\"] is [batch_size, num_peaks, hidden_size]\n",
    "\n",
    "        # This prediction might need adjustment depending on how FormulaTransformer handles output\n",
    "        # If fragment prediction is per peak:\n",
    "        # pred_frag_fps = self.spectra_encoder[1](aux_out[\"peak_tensor\"]) # Shape: [batch_size, num_peaks, magma_modulo]\n",
    "        # If fragment prediction is based on global context (e.g., CLS token):\n",
    "        pred_frag_fps = self.spectra_encoder[1](encoder_output) # Shape: [batch_size, magma_modulo]\n",
    "        # ^^^ Choose the correct input for fragment_predictor based on MIST's design\n",
    "\n",
    "        aux_outputs = {\"pred_frag_fps\": pred_frag_fps}\n",
    "\n",
    "        output = self.spectra_encoder[2](encoder_output) # Shape: [batch_size, output_size]\n",
    "        aux_outputs[\"h0\"] = encoder_output\n",
    "\n",
    "        return output, aux_outputs\n",
    "\n",
    "class SpectrumProcessor:\n",
    "    \"\"\"Process raw spectral data directly without file reading.\"\"\"\n",
    "    # --- Using the class provided in the prompt ---\n",
    "    # Note: Removed file I/O for MAGMA for simplicity in this example skeleton\n",
    "    # You'll need to adapt _process_magma_file if using it.\n",
    "    # Added Path import\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import pandas as pd # Added import\n",
    "\n",
    "    cat_types = {\"frags\": 0, \"loss\": 1, \"ab_loss\": 2, \"cls\": 3}\n",
    "    num_inten_bins = 10\n",
    "    num_types = len(cat_types)\n",
    "    cls_type_map = cat_types # Renamed cls_type to avoid conflict\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        augment_data: bool = False,\n",
    "        augment_prob: float = 1,\n",
    "        remove_prob: float = 0.1,\n",
    "        remove_weights: str = \"uniform\",\n",
    "        inten_prob: float = 0.1,\n",
    "        cls_token_type: str = \"ms1\", # Renamed cls_type param\n",
    "        max_peaks: int = None,\n",
    "        inten_transform: str = \"float\",\n",
    "        magma_modulo: int = 512,\n",
    "        magma_aux_loss: bool = False,\n",
    "        magma_folder: str = None, # Keep this if you implement MAGMA loading\n",
    "    ):\n",
    "        self.cls_token_type = cls_token_type # Use renamed param\n",
    "        self.augment_data = augment_data\n",
    "        self.remove_prob = remove_prob\n",
    "        self.augment_prob = augment_prob\n",
    "        self.remove_weights = remove_weights\n",
    "        self.inten_prob = inten_prob\n",
    "        self.max_peaks = max_peaks\n",
    "        self.inten_transform = inten_transform\n",
    "        self.aug_nbits = magma_modulo\n",
    "        self.magma_aux_loss = magma_aux_loss\n",
    "        # self.magma_folder = Path(magma_folder) if magma_folder else None\n",
    "        # self.spec_name_to_magma_file = {}\n",
    "        # if self.magma_aux_loss and self.magma_folder:\n",
    "        #     self._initialize_magma_mapping()\n",
    "\n",
    "    # --- Add dummy utils for demonstration ---\n",
    "    class DummyUtils:\n",
    "        VALID_MONO_MASSES = np.random.rand(18) # Placeholder\n",
    "        ELEM_TO_IDX = {'C': 0, 'H': 1, 'N': 2, 'O': 3, 'P': 4, 'S': 5, 'F': 6, 'Cl': 7, 'Br': 8, 'I': 9, 'Si': 10, 'Se': 11, 'B': 12, 'Na': 13, 'K': 14, 'Mg': 15, 'Ca': 16, 'Fe': 17} # Example\n",
    "        NUM_ELEM = 18 # Example\n",
    "        ION_LST = ['[M+H]+', '[M+Na]+', '[M+K]+', '[M-H]-', '[M+Cl]-'] # Example\n",
    "        ION_TO_IDX = {ion: i for i, ion in enumerate(ION_LST)} # Example\n",
    "\n",
    "        @staticmethod\n",
    "        def formula_to_dense(formula_str):\n",
    "            # Dummy implementation - replace with actual formula parsing\n",
    "            vec = np.zeros(DummyUtils.NUM_ELEM)\n",
    "            if isinstance(formula_str, str) and 'C' in formula_str: # Basic check\n",
    "                 vec[DummyUtils.ELEM_TO_IDX['C']] = int(formula_str.split('C')[1].split('H')[0]) if 'H' in formula_str else int(formula_str.split('C')[1]) # Very basic parsing\n",
    "                 # Add more elements...\n",
    "            return vec\n",
    "\n",
    "        @staticmethod\n",
    "        def get_ion_idx(ion_str):\n",
    "            return DummyUtils.ION_TO_IDX.get(ion_str, 0) # Default to first ion if not found\n",
    "\n",
    "    utils = DummyUtils() # Instantiate dummy utils\n",
    "    # --- End dummy utils ---\n",
    "\n",
    "\n",
    "    def process_raw_spectrum(self, raw_spectral_json, spec_id=None, train_mode=False):\n",
    "        if isinstance(raw_spectral_json, str):\n",
    "            tree = json.loads(raw_spectral_json)\n",
    "        else:\n",
    "            tree = raw_spectral_json\n",
    "        if spec_id is None and \"name\" in tree:\n",
    "            spec_id = tree[\"name\"]\n",
    "        peak_dict = self._get_peak_dict_from_raw(tree)\n",
    "        if train_mode and self.augment_data:\n",
    "            augment_peak = np.random.random() < self.augment_prob\n",
    "            if augment_peak:\n",
    "                peak_dict = self.augment_peak_dict(peak_dict)\n",
    "        features = self._generate_features(peak_dict, spec_id)\n",
    "        return features\n",
    "\n",
    "    def _get_peak_dict_from_raw(self, tree: dict) -> dict:\n",
    "        root_form = tree[\"cand_form\"]\n",
    "        root_ion = tree[\"cand_ion\"]\n",
    "        # Handle cases where output_tbl might be missing or empty\n",
    "        output_tbl = tree.get(\"output_tbl\")\n",
    "        if output_tbl is None or not output_tbl.get(\"formula\"):\n",
    "             frags, intens, ions = [], [], []\n",
    "        else:\n",
    "             frags = output_tbl.get(\"formula\", [])\n",
    "             intens = output_tbl.get(\"ms2_inten\", [])\n",
    "             ions = output_tbl.get(\"ions\", [])\n",
    "\n",
    "        # Ensure all lists have the same length if extracted\n",
    "        min_len = min(len(frags), len(intens), len(ions))\n",
    "        frags, intens, ions = frags[:min_len], intens[:min_len], ions[:min_len]\n",
    "\n",
    "        out_dict = {\n",
    "            \"frags\": frags, \"intens\": intens, \"ions\": ions,\n",
    "            \"root_form\": root_form, \"root_ion\": root_ion,\n",
    "        }\n",
    "\n",
    "        if self.max_peaks is not None and len(intens) > 0:\n",
    "            inten_list = list(out_dict[\"intens\"])\n",
    "            new_order = np.argsort(inten_list)[::-1]\n",
    "            cutoff_ind = min(len(inten_list), self.max_peaks)\n",
    "            new_inds = new_order[:cutoff_ind]\n",
    "            out_dict[\"intens\"] = np.array(inten_list)[new_inds].tolist()\n",
    "            out_dict[\"frags\"] = np.array(out_dict[\"frags\"])[new_inds].tolist()\n",
    "            out_dict[\"ions\"] = np.array(out_dict[\"ions\"])[new_inds].tolist()\n",
    "        return out_dict\n",
    "\n",
    "    def augment_peak_dict(self, peak_dict: dict):\n",
    "        frags = np.array(peak_dict[\"frags\"])\n",
    "        intens = np.array(peak_dict[\"intens\"])\n",
    "        ions = np.array(peak_dict[\"ions\"])\n",
    "        if len(frags) == 0: return peak_dict\n",
    "\n",
    "        num_modify_peaks = len(frags)\n",
    "        keep_prob = 1 - self.remove_prob\n",
    "        num_to_keep = np.random.binomial(n=num_modify_peaks, p=keep_prob)\n",
    "        keep_inds = np.arange(num_modify_peaks)\n",
    "\n",
    "        if self.remove_weights == \"quadratic\":\n",
    "            keep_probs = intens.reshape(-1) ** 2 + 1e-9\n",
    "        elif self.remove_weights == \"uniform\":\n",
    "            keep_probs = np.ones(len(intens))\n",
    "        elif self.remove_weights == \"exp\":\n",
    "            keep_probs = np.exp(intens.reshape(-1) + 1e-5)\n",
    "        else: raise NotImplementedError()\n",
    "        keep_probs = keep_probs / (keep_probs.sum() + 1e-9) # Normalize safely\n",
    "\n",
    "        # Ensure num_to_keep is not greater than available peaks\n",
    "        num_to_keep = min(num_to_keep, len(keep_inds))\n",
    "        if num_to_keep > 0 and len(keep_inds) > 0:\n",
    "             ind_samples = np.random.choice(keep_inds, size=num_to_keep, replace=False, p=keep_probs)\n",
    "             frags, intens, ions = frags[ind_samples], intens[ind_samples], ions[ind_samples]\n",
    "        elif num_to_keep == 0: # Handle edge case where all peaks might be removed\n",
    "             frags, intens, ions = np.array([]), np.array([]), np.array([])\n",
    "        # else: # num_to_keep > 0 but len(keep_inds) == 0 (should not happen) - keep original\n",
    "\n",
    "        if len(intens) > 0: # Only scale if peaks remain\n",
    "            rescale_prob = np.random.random(len(intens))\n",
    "            inten_scalar_factor = np.random.normal(loc=1, scale=0.1, size=len(intens)) # Added scale\n",
    "            inten_scalar_factor[inten_scalar_factor <= 0] = 1e-6 # Avoid zero/negative\n",
    "            inten_scalar_factor[rescale_prob >= self.inten_prob] = 1\n",
    "            intens = intens * inten_scalar_factor\n",
    "            new_max = intens.max() + 1e-12\n",
    "            intens /= new_max\n",
    "\n",
    "        peak_dict[\"intens\"] = intens.tolist() # Convert back to list\n",
    "        peak_dict[\"frags\"] = frags.tolist()\n",
    "        peak_dict[\"ions\"] = ions.tolist()\n",
    "        return peak_dict\n",
    "\n",
    "    def _process_magma_file(self, spec_name, mz_vec, forms_vec):\n",
    "        # Dummy implementation - returns fingerprints of -1\n",
    "        # Replace with your actual MAGMA file processing if needed\n",
    "        num_peaks = forms_vec.shape[0] if isinstance(forms_vec, np.ndarray) else len(forms_vec)\n",
    "        fingerprints = np.full((num_peaks, self.aug_nbits), -1.0)\n",
    "        # Your MAGMA loading logic here...\n",
    "        # Example: Set some dummy fingerprints for the non-CLS token if aux loss is on\n",
    "        if self.magma_aux_loss and num_peaks > 1:\n",
    "             # Find the CLS token index (assuming it's the last one)\n",
    "             cls_idx = -1\n",
    "             if self.cls_token_type in [\"ms1\", \"zeros\"]:\n",
    "                 cls_idx = num_peaks -1\n",
    "\n",
    "             for i in range(num_peaks):\n",
    "                 if i != cls_idx: # Don't assign FP to CLS token\n",
    "                     # Assign a dummy FP (e.g., mostly zeros with a few ones)\n",
    "                     fp = np.zeros(self.aug_nbits)\n",
    "                     num_ones = np.random.randint(1, min(10, self.aug_nbits)) # Few random bits on\n",
    "                     one_indices = np.random.choice(self.aug_nbits, num_ones, replace=False)\n",
    "                     fp[one_indices] = 1\n",
    "                     fingerprints[i, :] = fp\n",
    "\n",
    "        return fingerprints\n",
    "\n",
    "\n",
    "    def _generate_features(self, peak_dict: dict, spec_name=None):\n",
    "        # import utils # Use the dummy utils defined above\n",
    "        utils = self.utils\n",
    "\n",
    "        root = peak_dict[\"root_form\"]\n",
    "        forms_vec = [utils.formula_to_dense(i) for i in peak_dict[\"frags\"]]\n",
    "        if not forms_vec: # Handle empty list\n",
    "            mz_vec = []\n",
    "            forms_vec_arr = np.empty((0, utils.NUM_ELEM)) # Use correct shape\n",
    "        else:\n",
    "            forms_vec_arr = np.array(forms_vec)\n",
    "            mz_vec = (forms_vec_arr * utils.VALID_MONO_MASSES).sum(-1).tolist()\n",
    "\n",
    "        root_vec = utils.formula_to_dense(root)\n",
    "        root_ion = utils.get_ion_idx(peak_dict[\"root_ion\"])\n",
    "        root_mass = (root_vec * utils.VALID_MONO_MASSES).sum()\n",
    "        inten_vec = list(peak_dict[\"intens\"])\n",
    "        ion_vec = [utils.get_ion_idx(i) for i in peak_dict[\"ions\"]]\n",
    "        type_vec = len(forms_vec) * [self.cls_type_map[\"frags\"]]\n",
    "        instrument = 0\n",
    "\n",
    "        # Add classification token\n",
    "        cls_added = False\n",
    "        if self.cls_token_type == \"ms1\":\n",
    "            cls_ind = self.cls_type_map.get(\"cls\")\n",
    "            inten_vec.append(1.0)\n",
    "            type_vec.append(cls_ind)\n",
    "            # Append root_vec correctly\n",
    "            forms_list = forms_vec_arr.tolist()\n",
    "            forms_list.append(root_vec)\n",
    "            forms_vec_arr = np.array(forms_list)\n",
    "            mz_vec.append(root_mass)\n",
    "            ion_vec.append(root_ion)\n",
    "            cls_added = True\n",
    "        elif self.cls_token_type == \"zeros\":\n",
    "            cls_ind = self.cls_type_map.get(\"cls\")\n",
    "            inten_vec.append(0.0)\n",
    "            type_vec.append(cls_ind)\n",
    "            forms_list = forms_vec_arr.tolist()\n",
    "            forms_list.append(np.zeros_like(root_vec))\n",
    "            forms_vec_arr = np.array(forms_list)\n",
    "            mz_vec.append(0)\n",
    "            ion_vec.append(root_ion)\n",
    "            cls_added = True\n",
    "        # else: # No CLS token if type is not ms1 or zeros\n",
    "        #    pass\n",
    "\n",
    "        inten_vec = np.array(inten_vec)\n",
    "        if self.inten_transform == \"float\": self.inten_feats = 1\n",
    "        elif self.inten_transform == \"zero\": self.inten_feats = 1; inten_vec = np.zeros_like(inten_vec)\n",
    "        elif self.inten_transform == \"log\": self.inten_feats = 1; inten_vec = np.log(inten_vec + 1e-5)\n",
    "        elif self.inten_transform == \"cat\":\n",
    "            self.inten_feats = self.num_inten_bins\n",
    "            bins = np.linspace(0, 1, self.num_inten_bins)\n",
    "            inten_vec = np.digitize(inten_vec, bins)\n",
    "        else: raise NotImplementedError()\n",
    "\n",
    "        # Ensure forms_vec_arr is 2D even if empty or only CLS token added\n",
    "        if forms_vec_arr.ndim == 1 and forms_vec_arr.shape[0] > 0: # Only CLS token\n",
    "             forms_vec_arr = forms_vec_arr.reshape(1, -1)\n",
    "        elif len(forms_vec_arr) == 0: # Empty case\n",
    "             forms_vec_arr = np.empty((0, utils.NUM_ELEM))\n",
    "\n",
    "\n",
    "        # Process MAGMA fingerprints\n",
    "        # Pass the potentially updated forms_vec_arr\n",
    "        fingerprints = self._process_magma_file(spec_name, mz_vec, forms_vec_arr)\n",
    "\n",
    "        out_dict = {\n",
    "            \"peak_type\": np.array(type_vec),\n",
    "            \"form_vec\": forms_vec_arr, # Use the array\n",
    "            \"ion_vec\": np.array(ion_vec), # Convert to array\n",
    "            \"frag_intens\": inten_vec,\n",
    "            \"name\": spec_name,\n",
    "            \"magma_fps\": fingerprints,\n",
    "            \"magma_aux_loss\": self.magma_aux_loss,\n",
    "            \"instrument\": instrument, # Convert to array or keep as scalar? Assuming scalar\n",
    "            \"cls_added\": cls_added # Flag if CLS token was added\n",
    "        }\n",
    "        return out_dict\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def smiles_to_fingerprint(smiles: str, n_bits: int) -> np.ndarray:\n",
    "    \"\"\"Generates a Morgan fingerprint from a SMILES string.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"Warning: RDKit could not parse SMILES: {smiles}\")\n",
    "            return np.zeros(n_bits, dtype=np.float32)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating fingerprint for {smiles}: {e}\")\n",
    "        return np.zeros(n_bits, dtype=np.float32)\n",
    "\n",
    "def masked_bce_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculates BCE loss, ignoring targets where value is -1.\"\"\"\n",
    "    mask = target != -1\n",
    "    target_masked = target[mask]\n",
    "    pred_masked = pred[mask]\n",
    "    if target_masked.numel() == 0: # Handle cases where mask removes all elements\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True) # Return zero loss but allow grad flow\n",
    "    # Use BCEWithLogitsLoss for stability if model output is logits (before sigmoid)\n",
    "    # If model output is probabilities (after sigmoid), use BCELoss\n",
    "    # Assuming pred is probabilities here based on SpectraEncoder's Sigmoid\n",
    "    loss = nn.BCELoss(reduction='mean')(pred_masked, target_masked)\n",
    "    # loss = nn.BCEWithLogitsLoss(reduction='mean')(pred_masked, target_masked) # If preds are logits\n",
    "    return loss\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle padding and batching.\n",
    "    This needs to be adapted based on how FormulaTransformer expects input.\n",
    "    Assuming padding is needed for form_vec, peak_type, etc.\n",
    "    \"\"\"\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    collated = {}\n",
    "    keys_to_pad = ['form_vec', 'peak_type', 'ion_vec', 'frag_intens', 'magma_fps']\n",
    "    keys_to_stack = ['target_fp'] # Assuming target_fp is already fixed size\n",
    "    other_keys = ['name', 'magma_aux_loss', 'instrument', 'cls_added'] # Non-tensor or scalar data\n",
    "\n",
    "    # Find max sequence length in batch for padding\n",
    "    max_len = 0\n",
    "    if batch:\n",
    "         max_len = max(item['form_vec'].shape[0] for item in batch if 'form_vec' in item and item['form_vec'].ndim > 0)\n",
    "\n",
    "    # Pad and batch tensor data\n",
    "    for key in keys_to_pad:\n",
    "        # Ensure data exists and is numpy array before converting to tensor\n",
    "        sequences = [torch.from_numpy(item[key]) for item in batch if key in item and isinstance(item[key], np.ndarray) and item[key].size > 0]\n",
    "        if sequences:\n",
    "             # Handle potential 1D vs 2D arrays after processing\n",
    "             if sequences[0].ndim == 1: # e.g., peak_type, ion_vec, frag_intens\n",
    "                 # Pad 1D tensors\n",
    "                 collated[key] = pad_sequence(sequences, batch_first=True, padding_value=0) # Use 0 for padding types/ions/intens? Check MIST paper.\n",
    "             elif sequences[0].ndim == 2: # e.g., form_vec, magma_fps\n",
    "                 # Pad 2D tensors\n",
    "                 collated[key] = pad_sequence(sequences, batch_first=True, padding_value=0.0) # Pad formulas/fps with 0.0\n",
    "             else:\n",
    "                 print(f\"Warning: Unexpected tensor dimension for key {key}: {sequences[0].ndim}\")\n",
    "        else:\n",
    "             # Handle cases where a key might be missing or empty across the batch\n",
    "             # Create an empty tensor or handle appropriately downstream\n",
    "             # Example: Create tensor with shape (batch_size, 0) or (batch_size, 0, feature_dim)\n",
    "             batch_size = len(batch)\n",
    "             if key == 'form_vec': shape = (batch_size, 0, SpectrumProcessor.utils.NUM_ELEM)\n",
    "             elif key == 'magma_fps': shape = (batch_size, 0, batch[0]['magma_fps'].shape[1] if batch and 'magma_fps' in batch[0] and batch[0]['magma_fps'].ndim==2 else 512) # Use configured size\n",
    "             elif key in ['peak_type', 'ion_vec', 'frag_intens']: shape = (batch_size, 0)\n",
    "             else: shape = (batch_size, 0) # Default fallback\n",
    "             collated[key] = torch.empty(shape)\n",
    "\n",
    "\n",
    "    # Stack fixed-size tensors\n",
    "    for key in keys_to_stack:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = torch.stack([torch.from_numpy(item[key]) for item in batch], dim=0)\n",
    "\n",
    "    # Collect other data (non-tensor)\n",
    "    for key in other_keys:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "\n",
    "    # Add attention mask based on padding (assuming 0 padding for peak_type)\n",
    "    if 'peak_type' in collated:\n",
    "        # Mask should be True where there is actual data, False for padding\n",
    "        # Assuming padding value is 0 and actual types are non-zero (or handle cls_type=0 specifically)\n",
    "        # A safer mask might be based on sequence lengths before padding.\n",
    "        # Let's create a mask based on non-zero sum across formula vector dim\n",
    "        if 'form_vec' in collated and collated['form_vec'].numel() > 0:\n",
    "             # Sum across the element dimension (dim 2)\n",
    "             # Mask is True where the sum is non-zero (actual peak data)\n",
    "             attention_mask = torch.sum(collated['form_vec'], dim=2) != 0\n",
    "             collated['attention_mask'] = attention_mask # Shape: [batch_size, seq_len]\n",
    "        elif max_len > 0 : # Fallback if form_vec is empty but padding happened\n",
    "             # Create mask based on original lengths if available, otherwise assume all valid up to max_len\n",
    "             # This part needs careful implementation based on how lengths are tracked\n",
    "             collated['attention_mask'] = torch.ones(len(batch), max_len, dtype=torch.bool) # Placeholder mask\n",
    "        else: # No sequences\n",
    "             collated['attention_mask'] = torch.empty(len(batch), 0, dtype=torch.bool)\n",
    "\n",
    "\n",
    "    # Ensure all necessary keys for the model are present, even if empty\n",
    "    required_keys = ['form_vec', 'peak_type', 'ion_vec', 'frag_intens', 'attention_mask', 'target_fp']\n",
    "    if batch and batch[0].get('magma_aux_loss', False):\n",
    "        required_keys.append('magma_fps')\n",
    "\n",
    "    for key in required_keys:\n",
    "        if key not in collated:\n",
    "             # Handle missing keys, e.g., create empty tensors\n",
    "             print(f\"Warning: Key '{key}' missing in collated batch.\")\n",
    "             if key == 'target_fp': collated[key] = torch.empty((len(batch), 4096)) # Placeholder size\n",
    "             # Add similar handling for other keys if necessary\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "# --- Dataset Definition ---\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, data_file, spectrum_processor, target_fp_size, is_train=False):\n",
    "        self.processor = spectrum_processor\n",
    "        self.target_fp_size = target_fp_size\n",
    "        self.is_train = is_train\n",
    "        # Load your data (e.g., from TSV)\n",
    "        # Example assuming TSV: ID SMILES JSON_DATA ...\n",
    "        self.data = pd.read_csv(data_file, sep='\\t', header=None) # Adjust as needed\n",
    "        # Filter out rows where RDKit fails on SMILES?\n",
    "        # self.data = self.data[self.data[1].apply(lambda x: Chem.MolFromSmiles(x) is not None)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        spec_id = str(row[0])\n",
    "        smiles = str(row[1])\n",
    "        raw_spec_json = str(row[2]) # Assuming JSON is in the 3rd column\n",
    "\n",
    "        # Process spectrum\n",
    "        # Use train_mode for augmentation during training\n",
    "        spec_features = self.processor.process_raw_spectrum(\n",
    "            raw_spec_json, spec_id=spec_id, train_mode=self.is_train\n",
    "        )\n",
    "\n",
    "        # Generate target fingerprint\n",
    "        target_fp = smiles_to_fingerprint(smiles, self.target_fp_size)\n",
    "\n",
    "        # Combine features and target\n",
    "        item = {**spec_features, 'target_fp': target_fp}\n",
    "        return item\n",
    "\n",
    "# --- Training Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model Hyperparameters (match SpectraEncoder defaults or your config)\n",
    "OUTPUT_SIZE = 4096\n",
    "HIDDEN_SIZE = 50 # Example, adjust based on MIST paper/config\n",
    "SPECTRA_DROPOUT = 0.1 # Example\n",
    "TOP_LAYERS = 2 # Example\n",
    "MAGMA_MODULO = 2048 # Example\n",
    "FORM_EMBEDDER = \"float\" # Or 'abs', 'elec', etc.\n",
    "# Add other FormulaTransformer kwargs if needed (e.g., nhead, num_encoder_layers)\n",
    "FORMULA_TRANSFORMER_KWARGS = {\"nhead\": 2, \"num_encoder_layers\": 2, \"dim_feedforward\": 128} # Example\n",
    "\n",
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "TRAIN_DATA = \"path/to/your/train_data.tsv\" # Replace with your data path\n",
    "VAL_DATA = \"path/to/your/val_data.tsv\"   # Replace with your data path\n",
    "MAGMA_FOLDER = \"path/to/your/magma_files\" # Optional: Replace if using MAGMA aux loss\n",
    "USE_MAGMA_AUX_LOSS = True # Set to True if using MAGMA fingerprints\n",
    "MAGMA_LOSS_WEIGHT = 0.2 # Weight for the auxiliary loss\n",
    "\n",
    "# --- Initialization ---\n",
    "# Initialize Spectrum Processor\n",
    "# Ensure magma_aux_loss matches USE_MAGMA_AUX_LOSS\n",
    "processor = SpectrumProcessor(\n",
    "    augment_data=True, # Enable augmentation for training data\n",
    "    cls_token_type=\"ms1\",\n",
    "    max_peaks=500, # Example: Limit number of peaks\n",
    "    magma_modulo=MAGMA_MODULO,\n",
    "    magma_aux_loss=USE_MAGMA_AUX_LOSS,\n",
    "    magma_folder=MAGMA_FOLDER if USE_MAGMA_AUX_LOSS else None\n",
    ")\n",
    "\n",
    "# Initialize Datasets and DataLoaders\n",
    "train_dataset = SpectraDataset(TRAIN_DATA, processor, OUTPUT_SIZE, is_train=True)\n",
    "val_dataset = SpectraDataset(VAL_DATA, processor, OUTPUT_SIZE, is_train=False) # No augmentation for validation\n",
    "\n",
    "# Use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize Model\n",
    "model = SpectraEncoder(\n",
    "    form_embedder=FORM_EMBEDDER,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    spectra_dropout=SPECTRA_DROPOUT,\n",
    "    top_layers=TOP_LAYERS,\n",
    "    magma_modulo=MAGMA_MODULO,\n",
    "    **FORMULA_TRANSFORMER_KWARGS # Pass transformer specific args\n",
    ").to(DEVICE)\n",
    "\n",
    "# Initialize Loss Functions\n",
    "criterion_main = nn.BCELoss() # Use BCELoss because model has Sigmoid\n",
    "# criterion_main = nn.BCEWithLogitsLoss() # Use this if you remove the final Sigmoid from the model\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_main_loss = 0\n",
    "    total_aux_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Move batch to device (handle non-tensor data appropriately)\n",
    "        batch_gpu = {}\n",
    "        for key, value in batch.items():\n",
    "             if isinstance(value, torch.Tensor):\n",
    "                 batch_gpu[key] = value.to(DEVICE)\n",
    "             else:\n",
    "                 batch_gpu[key] = value # Keep non-tensors like names, flags as they are\n",
    "\n",
    "        # Skip batch if essential tensors are empty after collation/filtering\n",
    "        if not batch_gpu.get('form_vec', torch.empty(0)).numel() or not batch_gpu.get('target_fp', torch.empty(0)).numel():\n",
    "             print(f\"Skipping empty batch {i}\")\n",
    "             continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            output, aux_outputs = model(batch_gpu) # Pass the gpu batch\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass on batch {i}: {e}\")\n",
    "            # Optionally print batch details for debugging\n",
    "            # for key, value in batch_gpu.items():\n",
    "            #     if isinstance(value, torch.Tensor):\n",
    "            #         print(f\"Batch key: {key}, Shape: {value.shape}, Device: {value.device}\")\n",
    "            #     else:\n",
    "            #         print(f\"Batch key: {key}, Value: {value}\")\n",
    "            continue # Skip this batch\n",
    "\n",
    "        # Calculate Main Loss\n",
    "        target_fp = batch_gpu['target_fp']\n",
    "        main_loss = criterion_main(output, target_fp)\n",
    "\n",
    "        # Calculate Auxiliary Loss (if enabled)\n",
    "        aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        if USE_MAGMA_AUX_LOSS and 'pred_frag_fps' in aux_outputs and 'magma_fps' in batch_gpu and batch_gpu['magma_fps'].numel() > 0:\n",
    "            pred_frag_fps = aux_outputs['pred_frag_fps'] # Shape: [batch, (num_peaks,) magma_modulo]\n",
    "            target_magma_fps = batch_gpu['magma_fps']   # Shape: [batch, num_peaks, magma_modulo]\n",
    "\n",
    "            # --- IMPORTANT ---\n",
    "            # Ensure pred_frag_fps and target_magma_fps align.\n",
    "            # If pred_frag_fps is [batch, magma_modulo] (global prediction),\n",
    "            # you cannot directly compare it to per-peak target_magma_fps.\n",
    "            # The MIST paper/code needs clarification on how aux loss is computed.\n",
    "            # Assuming pred_frag_fps is per-peak for this example:\n",
    "            # Shape: [batch, num_peaks, magma_modulo]\n",
    "            # Make sure the shapes match before calling masked_bce_loss.\n",
    "            # If shapes mismatch, you need to adapt the model or loss calculation.\n",
    "\n",
    "            # Example check (adjust based on actual model output shape):\n",
    "            if pred_frag_fps.shape == target_magma_fps.shape:\n",
    "                 # Apply sigmoid if fragment_predictor doesn't have one\n",
    "                 pred_frag_fps_sig = torch.sigmoid(pred_frag_fps) # Assuming logits output\n",
    "                 aux_loss = masked_bce_loss(pred_frag_fps_sig, target_magma_fps)\n",
    "            else:\n",
    "                 print(f\"Warning: Shape mismatch for MAGMA loss. Pred: {pred_frag_fps.shape}, Target: {target_magma_fps.shape}. Skipping aux loss.\")\n",
    "                 aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "\n",
    "        # Combine Losses\n",
    "        total_loss = main_loss + MAGMA_LOSS_WEIGHT * aux_loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        try:\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during backward/step on batch {i}: {e}\")\n",
    "            # Potentially clear gradients if step failed\n",
    "            optimizer.zero_grad()\n",
    "            continue # Skip this batch update\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_main_loss += main_loss.item()\n",
    "        if USE_MAGMA_AUX_LOSS:\n",
    "            total_aux_loss += aux_loss.item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss = total_train_loss / (i + 1)\n",
    "            avg_main_loss = total_main_loss / (i + 1)\n",
    "            avg_aux_loss = total_aux_loss / (i + 1) if USE_MAGMA_AUX_LOSS else 0\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Avg Loss: {avg_loss:.4f}, Avg Main Loss: {avg_main_loss:.4f}, Avg Aux Loss: {avg_aux_loss:.4f}')\n",
    "\n",
    "    print(f'--- Epoch {epoch+1} Training Finished ---')\n",
    "    print(f'Average Training Loss: {total_train_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_main_loss = 0\n",
    "    total_val_aux_loss = 0\n",
    "    # Add metric calculation (e.g., Tanimoto)\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_gpu = {}\n",
    "            for key, value in batch.items():\n",
    "                 if isinstance(value, torch.Tensor):\n",
    "                     batch_gpu[key] = value.to(DEVICE)\n",
    "                 else:\n",
    "                     batch_gpu[key] = value\n",
    "\n",
    "            if not batch_gpu.get('form_vec', torch.empty(0)).numel() or not batch_gpu.get('target_fp', torch.empty(0)).numel():\n",
    "                 print(f\"Skipping empty validation batch\")\n",
    "                 continue\n",
    "\n",
    "            try:\n",
    "                output, aux_outputs = model(batch_gpu)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during validation forward pass: {e}\")\n",
    "                 continue\n",
    "\n",
    "            target_fp = batch_gpu['target_fp']\n",
    "            main_loss = criterion_main(output, target_fp)\n",
    "\n",
    "            aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "            if USE_MAGMA_AUX_LOSS and 'pred_frag_fps' in aux_outputs and 'magma_fps' in batch_gpu and batch_gpu['magma_fps'].numel() > 0:\n",
    "                 pred_frag_fps = aux_outputs['pred_frag_fps']\n",
    "                 target_magma_fps = batch_gpu['magma_fps']\n",
    "                 if pred_frag_fps.shape == target_magma_fps.shape:\n",
    "                     pred_frag_fps_sig = torch.sigmoid(pred_frag_fps)\n",
    "                     aux_loss = masked_bce_loss(pred_frag_fps_sig, target_magma_fps)\n",
    "                 else:\n",
    "                     aux_loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "\n",
    "            total_loss = main_loss + MAGMA_LOSS_WEIGHT * aux_loss\n",
    "            total_val_loss += total_loss.item()\n",
    "            total_val_main_loss += main_loss.item()\n",
    "            if USE_MAGMA_AUX_LOSS:\n",
    "                total_val_aux_loss += aux_loss.item()\n",
    "\n",
    "            # Store predictions and targets for metrics\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(target_fp.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    avg_val_main_loss = total_val_main_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    avg_val_aux_loss = total_val_aux_loss / len(val_loader) if len(val_loader) > 0 and USE_MAGMA_AUX_LOSS else 0\n",
    "\n",
    "    print(f'--- Epoch {epoch+1} Validation Finished ---')\n",
    "    print(f'Average Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Average Val Main Loss: {avg_val_main_loss:.4f}')\n",
    "    if USE_MAGMA_AUX_LOSS:\n",
    "        print(f'Average Val Aux Loss: {avg_val_aux_loss:.4f}')\n",
    "\n",
    "    # Calculate Tanimoto Similarity (Example)\n",
    "    if all_preds and all_targets:\n",
    "        all_preds_cat = np.concatenate(all_preds, axis=0)\n",
    "        all_targets_cat = np.concatenate(all_targets, axis=0)\n",
    "        # Binarize predictions (e.g., threshold at 0.5)\n",
    "        preds_binary = (all_preds_cat > 0.5).astype(int)\n",
    "        targets_binary = all_targets_cat.astype(int) # Target should already be binary\n",
    "\n",
    "        intersection = np.sum(preds_binary * targets_binary, axis=1)\n",
    "        union = np.sum(np.logical_or(preds_binary, targets_binary).astype(int), axis=1)\n",
    "        # Avoid division by zero for empty fingerprints\n",
    "        tanimoto_scores = np.divide(intersection, union, out=np.zeros_like(intersection, dtype=float), where=union!=0)\n",
    "        avg_tanimoto = np.mean(tanimoto_scores)\n",
    "        print(f'Average Tanimoto Similarity: {avg_tanimoto:.4f}')\n",
    "\n",
    "    # Add model saving logic here (e.g., save best model based on val loss or Tanimoto)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
